@software{mlx2023,
  author = {Awni Hannun and Jagrit Digani and Angelos Katharopoulos and Ronan Collobert},
  title = {{MLX}: Efficient and flexible machine learning on Apple silicon},
  url = {https://github.com/ml-explore},
  version = {0.0},
  year = {2023},
}

@software{dalle3,
    author = {OpenAI},
    title = {{DALLÂ·E} 3: Prompt: "Photorealistic version of a Llama with 4 legs wearing a park ranger vest and a park ranger hat wearing sunglasses"},
    journal = {OpenAI Blog},
    year = {2024},
    note = {This image was generated with the assistance of AI},
}

@software{llama3,
    author = {Meta},
    title = {{Llama-3-8B} Base Model used for fine-tuning},
    url = {https://huggingface.co/meta-llama/Meta-Llama-3-8B},
    year = {2024},
}

@article{wenqi,
  author          = {Wenqi Glantz},
  journal         = {Towards Data Science},
  title           = {Democratizing LLMs: 4-bit Quantization for Optimal LLM Inference},
  year            = {2024}
}

@misc{dettmers2023qlora,
      title={QLoRA: Efficient Finetuning of Quantized LLMs}, 
      author={Tim Dettmers and Artidoro Pagnoni and Ari Holtzman and Luke Zettlemoyer},
      year={2023},
      eprint={2305.14314},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{hu2021lora,
      title={LoRA: Low-Rank Adaptation of Large Language Models}, 
      author={Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
      year={2021},
      eprint={2106.09685},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{openaitokenizer,
  title        = {Tokenizer Tool},
  author       = {{OpenAI}},
  year         = 2024,
  url = {https://platform.openai.com/tokenizer},
  note         = {Tokenizer example},
}
